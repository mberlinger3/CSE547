{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CSE547-Colab9.ipynb","provenance":[{"file_id":"15i4FVftbhz1IOSu8GMOsv-lgmQzqhWjJ","timestamp":1591127233088}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"y6elcu3cX2AO","colab_type":"text"},"source":["#CSE547 - Colab 9\n","#Causal Inference"]},{"cell_type":"markdown","metadata":{"id":"LbKUYUumZr6i","colab_type":"text"},"source":["# Set up"]},{"cell_type":"markdown","metadata":{"id":"PUUjUvXe3Sjk","colab_type":"text"},"source":["Let's authenticate a Google Drive client to download the file we will be processing.\n","\n","**Make sure to follow the interactive instructions.**"]},{"cell_type":"code","metadata":{"id":"mW6NrbFLpDK1","colab_type":"code","colab":{}},"source":["!pip install -U -q PyDrive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lRElWs_x2mGh","colab_type":"code","colab":{}},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QHsFTGUy2n1c","colab_type":"code","colab":{}},"source":["id='1Ffk6E_z-JE7wowTVC6em1HtzzKbdW1yh'\n","downloaded = drive.CreateFile({'id': id})\n","downloaded.GetContentFile('running.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwtlO4_m_LbQ","colab_type":"text"},"source":["If you executed the cells above, you should be able to see the dataset we will use for this Colab under the \"Files\" tab on the left panel.\n","\n","Next, we import some of the common libraries for our task."]},{"cell_type":"code","metadata":{"id":"ZH2MGg81PCJb","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import statsmodels.api as sm\n","from statsmodels.api import OLS\n","from sklearn.neighbors import NearestNeighbors"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v8HLb9yUr06Q","colab_type":"text"},"source":["# Your Task"]},{"cell_type":"markdown","metadata":{"id":"f2fU4qlnYObP","colab_type":"text"},"source":["In this colab, you will work on a fictitious data that aims to demonstrate some of the common challenges and mistakes in estimating the Average Treatment Effects (ATE). Below is the description of the fictitious data:"]},{"cell_type":"markdown","metadata":{"id":"Pef4_uwevT1b","colab_type":"text"},"source":["*SuperShoes* Company recently released a new product named *Lightning Shoes*, and claimed that wearing the shoes will significantly speed up how fast a person can run. A skeptical researcher decided to conduct a study to test the credibility of this claim. To do this, he recruited 20,000 people who may or may not have bought the lightning shoes, and asked them to record how many meters they can sprint in one minute. For those who have bought the lightning shoes, they would need to wear the shoes to do the test run. The researcher also collected a variety of data on the participants' demographic and health-related information. The final sample the researcher obtained is the  **running.csv** dataset, with the following variables: \n","\n","*   **speed**: number of meters the participant runs in a minute.\n","*   **lightning**: whether the participant wore the lightning shoes for the run. 1 = yes, 0 = no.\n","*   **age**: age of the participant.\n","*   **gender**: 1 = male, 0 = female.\n","*   **veg**: 1 = vegetarian, 0 = non-vegetarian.\n","*   **exercise**: exercise level on a scale of [1, 10]\n","*   **muscle**: muscle level on a scale of [1, 10]\n","*  **stress**: stress level on a scale of [1, 10]\n","*   **heart**: heart rate that is recorded after the 1 minute run.\n","\n","We are interested in estimating the treatment effect of wearing the lightning shoes (i.e. the cause) on the speed people can run (i.e. the outcome).\n"]},{"cell_type":"markdown","metadata":{"id":"jyJ9g0seYEOF","colab_type":"text"},"source":["# Data Overview"]},{"cell_type":"code","metadata":{"id":"NAt1neSme3cz","colab_type":"code","colab":{}},"source":["# load the data\n","df = pd.read_csv(\"running.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y0yz4wBfygnp","colab_type":"code","colab":{}},"source":["# have a view on the data \n","df.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HxYnUk24IV3P","colab_type":"text"},"source":["## Q1: Calculate the unadjusted ATE.  "]},{"cell_type":"markdown","metadata":{"id":"f_exoe9kfjQ5","colab_type":"text"},"source":["First, calculate the difference in the average speed between people who wear the lightning shoes (i.e. treatment group) and people do not (i.e. control group). Let's call this the unadjusted ATE. **Report the unadjusted ATE rounded to 3 decimal places.**   \n"]},{"cell_type":"code","metadata":{"id":"8VW2WkcVJmYP","colab_type":"code","colab":{}},"source":["# YOUR CODE HERE \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JLIxLwoxhsPQ","colab_type":"text"},"source":["We can obtain the same result by running a simple linear regression where the independent variable is the treatment status (i.e. whether the participant wears the lightning shoes or not), and the outcome variable is the speed of running. The **OLS_estimate()** below uses the [statsmodels API](https://www.statsmodels.org/stable/index.html) to perform the linear regressions. Check whether the coefficient on lightning is the same as what you have obtained in Q1. "]},{"cell_type":"code","metadata":{"id":"sFF8u1mH5upb","colab_type":"code","colab":{}},"source":["def OLS_estimate(outcome, covariates):\n","    covariates = sm.add_constant(covariates) # adding a constant \n","    model = OLS(outcome, covariates)\n","    result = model.fit()\n","    print(result.summary())\n","\n","OLS_estimate(df['speed'], df[['lightning']])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_vvHV2x8k38S","colab_type":"text"},"source":["The result so far would indicate that there is no statistically significant effect of wearing the shoes on improving how fast a person can run. If anything, it is a small negative effect.  However, this result is most likely  biased. **In fact, the true effect is known to be 10**. What has gone wrong? "]},{"cell_type":"markdown","metadata":{"id":"GLLNGZ8eKPMf","colab_type":"text"},"source":["The main problem here is that people who choose to obtain the shoes are different from people who choose not to obtain the lightning shoes. This is called the **selection bias**, where people *select themselves into the treatment group* due to some other factors that also affect how fast they can run. Those factors are called the **confounders**. To see this, let's calculate the sample averages of the covariates between the two groups.   \n","\n"]},{"cell_type":"code","metadata":{"id":"hYksHRw0NvnZ","colab_type":"code","colab":{}},"source":["df.groupby('lightning')['exercise', 'age', 'stress', 'gender', 'muscle', 'veg', 'heart'].agg(['mean'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lEB6hFt9oM-u","colab_type":"text"},"source":["This shows that people in the treatment group are on average older, exercise less, and have less muscle. In the next section, we control for those variables and see how they change our estimates.  "]},{"cell_type":"markdown","metadata":{"id":"FXT_TDLC4_jU","colab_type":"text"},"source":["# Linear Regressions"]},{"cell_type":"markdown","metadata":{"id":"Ur1R0cVh2Uq9","colab_type":"text"},"source":["In this section, run a set of linear regressions using the **OLS_estimate()** function, with the following additional covariates besides the lightning variable: \n","\n","*   Specification 1 (S1): exercise, age, gender, veg. \n","*   Specification 2 (S2): include all variables. \n","*   Specification 3 (S3): exercise, age, gender, stress, muscle.\n"]},{"cell_type":"markdown","metadata":{"id":"4hpeOM5HhItP","colab_type":"text"},"source":["## Q2: What are the estimated ATEs from these three specifications? "]},{"cell_type":"code","metadata":{"id":"fyWI8AY0qxc1","colab_type":"code","colab":{}},"source":["# YOUR CODE HERE "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hGfqfV5Tq4xf","colab_type":"text"},"source":["As you can see from the results, apart from S3, which is our true model, S1 and S2 both severely under-estimate the true effect of 10, *even though the coefficeints are both statiscally significant (i.e. with extremely small p-value)*. This demonstrates the fact that even if the estimates are statistically significant and the R-squared score is nearly perfect, as in this case, it does not indicate we have obtained the unbiased estimates. \n","\n","\n","This also demonstrates that including more variables do not necessarily reduce our biases. In S1, the result is under-estimated because we fail to control for some of the confounders such as stress and muscle. However, controlling for too many varialbes, such as in S3, is also wrong. What happened in S3 is that we have a **collider** in the data, where the heart rate is affected by both the outcome (i.e. speed) and the treatment variable (i.e. lightning). Controlling for the collider would lead to distorted associations between the treatment and outcome. For more details on this problem, see [collider](https://en.wikipedia.org/wiki/Collider_(statistics))). \n","\n"]},{"cell_type":"markdown","metadata":{"id":"HSpI5K15jWnE","colab_type":"text"},"source":["# Propensity Score Matching\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"m-Tt1P0Iw36q","colab_type":"text"},"source":["One method to tackle the problem is **propensity score matching (PSM)**. The idea is to reduce the biases by comparing outcomes between the treated and control subjects who have similar propensity to be treated (i.e. propensity scores). In this section, you will implement three types of matching: **(1) nearest-neighbor matching (2) stratification matching, and (3) inverse probability weighting (IPW) matching**. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IEc9XZRXMY_V","colab_type":"text"},"source":["The **gen_pscore()** function generates the propensity scores by performing a logistic regression. It also plots the distributions of generated propensity scores across the groups. We will use this function to generate the propensity scores using exercise, age, stress, gender, muscle and veg as covariates."]},{"cell_type":"code","metadata":{"id":"XYoXISyV17Kv","colab_type":"code","colab":{}},"source":["def gen_pscore(data, outcome, covariates):\n","    model = sm.Logit(outcome, covariates)\n","    result = model.fit()\n","    data['pscore'] = result.predict(covariates)\n","    data.groupby(['lightning']).pscore.plot(kind='hist', bins=20, alpha=0.8, legend=True)\n","\n","gen_pscore(df, df['lightning'], df[[ 'exercise', 'age', 'stress', 'gender', 'muscle', 'veg']] )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uLAK176N1qfp","colab_type":"text"},"source":["The table below summarizes the distribution of the propensity scores for the two groups respectively. "]},{"cell_type":"code","metadata":{"id":"myJ_3IX1GOeQ","colab_type":"code","colab":{}},"source":["df.groupby(['lightning']).pscore.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XHy4QVVx6GfR","colab_type":"text"},"source":["Notice that some scores lie outside of the common ranges. The **trim()** function below trims the sample such that the treatment and control group share the common support.\n"]},{"cell_type":"code","metadata":{"id":"5-nqDJCAFyBH","colab_type":"code","colab":{}},"source":["def trim(data):\n","  control_data = data[data.lightning == 0]\n","  treat_data = data[data.lightning == 1]\n","\n","  min_control, min_treat = control_data.pscore.min(), treat_data.pscore.min()\n","  max_control, max_treat = control_data.pscore.max(), treat_data.pscore.max()\n","\n","  min_support = max(min_control, min_treat)\n","  max_support = min(max_control, max_treat)\n","\n","  trim_data = data.loc[((data.pscore >= min_support) & (data.pscore <= max_support)),:]\n","  \n","  return trim_data\n","\n","trim_df = trim(df)\n","trim_df.groupby(['lightning']).pscore.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9_x4bHbF562R","colab_type":"text"},"source":["## Q3: Perform Nearest-Neighbor Matching and report the ATE estimate with 3 decimal places."]},{"cell_type":"markdown","metadata":{"id":"q3NOgFpnOHAp","colab_type":"text"},"source":["The function **Nearest_Neighbor_Pair()** below finds one control unit that has the nearest propensity score for each treatment unit (with replacement), and returns a dataframe that contains only the control units that are matched. \n","\n","Use this function to compute the average treatment effect by calculating the mean of the differences in outcomes between the treated and controlled for each pair. Remember to use the trimmed data. "]},{"cell_type":"code","metadata":{"id":"f1b5KXNkOGCX","colab_type":"code","colab":{}},"source":["def Nearest_Neighbor_Pair(treated_df, non_treated_df):\n","    treated_x = treated_df['pscore'].values\n","    non_treated_x = non_treated_df['pscore'].values\n","\n","    nbrs = NearestNeighbors(n_neighbors=1).fit(np.expand_dims(non_treated_x, axis=1))\n","    distances, indices = nbrs.kneighbors(np.expand_dims(treated_x, axis=1))\n","    indices = indices.reshape(indices.shape[0])\n","    matched = non_treated_df.iloc[indices]\n","    return matched\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MlEDwyLQwt38","colab_type":"code","colab":{}},"source":["# YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rHgbkqH38_qP","colab_type":"text"},"source":["Note that what we have obtained is actually the **Average Treatment Effect on the Treated (ATT)**, since we are matching on the treated units. By doing so, we are implicitly assuming that the control unit we have picked as counterpart for each treated unit is a good approximation of what the outcome of the treated unit would be if it were not treated.\n","\n","In fact, the ATT does not necessarily have to be the same as ATE or ATU  (the average treatment effect on the untreated). **Since the true effect is constant across all the samples (i.e. constant treatment effect), the ATT is equal to ATE in this particular case.**  \n"]},{"cell_type":"markdown","metadata":{"id":"KgavCag-1abz","colab_type":"text"},"source":["## Q4: Perform stratification matching and report the estimate with 3 decimal places. "]},{"cell_type":"markdown","metadata":{"id":"aME3dOGd1yxB","colab_type":"text"},"source":["Perform a stratification matching with 10 equally spaced stratas, where strata = 1 if pscore is in (0, 0.1], strata = 2 if pscore is in (0,1, 0.2] and so on. Report the average treatment effect on the treated (ATT) using the following formula: \n","\n","$ATT_{strata} = \\sum_{s = 1}^{s = 10} \\frac{N^T_s}{N^T} (\\bar{Y}^{T}_s - \\bar{Y}^C_s)$\n","\n","where \n","$N^T$ is the number of treated in the whole sample, $N^T_s$ is the number of treated units in strata $s$, $\\bar{Y}^{T}_s - \\bar{Y}^C_s$ is the difference in sample averages of outcomes between the treated and control within strata $s$. Remember to use the trimmed data. \n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"FvcRLR_YKnvK","colab_type":"code","colab":{}},"source":["# YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_b1KOWp9FDa","colab_type":"text"},"source":["## Q5: Perform Inverse Probability Weighted (IPW) Matching  and report the estimate in 3 decimal places.  "]},{"cell_type":"markdown","metadata":{"id":"4Cs4MqpSims5","colab_type":"text"},"source":["We can also estimate the treament effect using the **Inverse Probability Weighted (IPW) Matching** method. The ATT from IPW matching is calculated as follows:\n","\n","$ATT_{IPW} = \\frac{1}{n}\\sum_{i}\\frac{T_i Y_i}{p_i} - \\frac{1}{n}\\sum_{i}\\frac{(1-T_i) Y_i}{1-p_i}$ \n","\n","where $T_i$ is 1 if i is treated, and 0 if not. $Y_i$ is the outcome of individual $i$ and $p_i$ is the propensity score for individual $i$.\n","\n","Compute the ATT from IPW matching. "]},{"cell_type":"code","metadata":{"id":"zdXxVcPDlBkH","colab_type":"code","colab":{}},"source":["# YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DuUwpRczvE1d","colab_type":"text"},"source":["In practice, you do not need to do all the estimations from scratch. There are various libraries/packages that implement these methods for you. For instance, check out the [causalinference](https://causalinferenceinpython.org/causalinference.html#module-causalinference.causal) package if you are interested."]},{"cell_type":"markdown","metadata":{"id":"NxBLMo0LfJwd","colab_type":"text"},"source":["Once you obtained the desired results, **head over to Gradescope and submit your solution for this Colab**!"]}]}